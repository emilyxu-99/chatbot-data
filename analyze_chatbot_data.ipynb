{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk \n",
    "from nltk.classify.textcat import TextCat \n",
    "from matplotlib.pyplot import plot \n",
    "from datascience import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fuzzywuzzy[speedup]\n",
      "  Using cached fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Processing ./.cache/pip/wheels/a8/da/22/2970e270912ba623ccac7d516b7411a820c8f2b4252463a605/python_Levenshtein-0.12.0-cp38-cp38-linux_x86_64.whl\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from python-levenshtein>=0.12; extra == \"speedup\"->fuzzywuzzy[speedup]) (49.6.0.post20201009)\n",
      "Installing collected packages: python-levenshtein, fuzzywuzzy\n",
      "Successfully installed fuzzywuzzy-0.18.0 python-levenshtein-0.12.0\n",
      "Requirement already satisfied: python-Levenshtein in /opt/conda/lib/python3.8/site-packages (0.12.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from python-Levenshtein) (49.6.0.post20201009)\n",
      "Processing ./.cache/pip/wheels/c5/01/a4/0160c55074707b535a6757a541842817d530d8080ca943a107/langid-1.1.6-py3-none-any.whl\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from langid) (1.18.5)\n",
      "Installing collected packages: langid\n",
      "Successfully installed langid-1.1.6\n"
     ]
    }
   ],
   "source": [
    "#fuzzywuzzy\n",
    "!pip3 install fuzzywuzzy[speedup]\n",
    "!pip install python-Levenshtein\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "#(py)torch\n",
    "#!pip install torch\n",
    "\n",
    "#langid (language identification)\n",
    "!pip install langid\n",
    "import langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#DO NOT RUN! This code creates a traditional table as seen in the datascience package but takes up too much memory. \n",
    "#I suggest running the code block after this one that generates chunks of 10 for each row of data.\n",
    "\n",
    "chatbots_data = Table.read_table(\"chatbots_data.csv\")\n",
    "chatbots_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 7)\n",
      "==================================================================\n",
      "              Created On     Flow Name  Merged Code  \\\n",
      "0  5/27/2020 10:31:06 PM    OB_NewUser          NaN   \n",
      "1   5/27/2020 7:38:52 PM  HLP_Feedback          NaN   \n",
      "\n",
      "                                             Message    Org Name  \\\n",
      "0                                            Kya kru   Chhaa Jaa   \n",
      "1  I like you\\nAnd I think improve not only about...  Big Sis V3   \n",
      "\n",
      "   Referred Flow                                  Uuid  \n",
      "0            NaN  915617a9-c0ae-47fc-8df0-54d72d4f41d9  \n",
      "1            NaN  148d099c-54eb-4182-961c-068eeda96691  \n",
      "==================================================================\n"
     ]
    }
   ],
   "source": [
    "#dividing the data into chunks of 10, because of memory issues (kernel keeps dying)\n",
    "#for example, this is what one chunk looks like\n",
    "chunksize = 10\n",
    "for chunk in pd.read_csv(\"chatbots_data.csv\", chunksize = chunksize):\n",
    "    print(chunk.shape)\n",
    "    print(\"=\"*66)\n",
    "    print(chunk.head(2))\n",
    "    print(\"=\"*66)\n",
    "    break\n",
    "    \n",
    "#appending chunks to chatbot_list, an empty array\n",
    "chatbot_list = []\n",
    "chunksize = 10\n",
    "for chunk in pd.read_csv(\"chatbots_data.csv\", chunksize = chunksize):\n",
    "    chatbot_list.append(chunk)\n",
    "\n",
    "#Combine all the chunks into one dataframe\n",
    "chatbots_data_chunks = pd.concat(chatbot_list, axis=0)\n",
    "chatbots_data_chunks = pd.DataFrame(data = chatbots_data_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleans the \"Message\" column\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "def preprocess(sentence):\n",
    "    sentence=str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    sentence=sentence.replace('{html}',\"\").replace('_',\"\")\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(rem_num)  \n",
    "    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
    "    stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "chatbots_data_chunks['Message']=chatbots_data_chunks['Message'].map(lambda s:preprocess(s)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Created On</th>\n",
       "      <th>Flow Name</th>\n",
       "      <th>Merged Code</th>\n",
       "      <th>Message</th>\n",
       "      <th>Org Name</th>\n",
       "      <th>Referred Flow</th>\n",
       "      <th>Uuid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5/27/2020 10:31:06 PM</td>\n",
       "      <td>OB_NewUser</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kya kru</td>\n",
       "      <td>Chhaa Jaa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>915617a9-c0ae-47fc-8df0-54d72d4f41d9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5/27/2020 7:38:52 PM</td>\n",
       "      <td>HLP_Feedback</td>\n",
       "      <td>NaN</td>\n",
       "      <td>like nand think improve sex topics nbut best</td>\n",
       "      <td>Big Sis V3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>148d099c-54eb-4182-961c-068eeda96691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5/27/2020 7:07:28 PM</td>\n",
       "      <td>OB_NewUser</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>Chhaa Jaa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b05c8625-c18e-4f3a-9054-f7260f2541c5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5/27/2020 6:34:17 PM</td>\n",
       "      <td>HLP_Feedback</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nihaiti bakwaas</td>\n",
       "      <td>Chhaa Jaa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>afd1ead9-90b8-40c5-8fd6-b543a9a6067b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5/27/2020 5:53:14 PM</td>\n",
       "      <td>HLP_Feedback</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mujhe apse bhut kuch seekhne mila</td>\n",
       "      <td>Chhaa Jaa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ddb1be9b-623d-4892-88c1-e441bf983103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82132</th>\n",
       "      <td>10/22/2020 4:00:18 AM</td>\n",
       "      <td>Lookup_unhandled</td>\n",
       "      <td>nullnull</td>\n",
       "      <td>topic</td>\n",
       "      <td>Big Sis V3</td>\n",
       "      <td>NEW MENU_heart of big sis</td>\n",
       "      <td>19491afa-307d-4ca2-b04a-89f5f21651fb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82133</th>\n",
       "      <td>10/22/2020 3:42:48 AM</td>\n",
       "      <td>Lookup_unhandled</td>\n",
       "      <td>nullnull</td>\n",
       "      <td>thank</td>\n",
       "      <td>Big Sis V3</td>\n",
       "      <td>FP_MS1_MSSA</td>\n",
       "      <td>cf79a515-0e43-480f-9885-3c90eac8fd86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82134</th>\n",
       "      <td>10/22/2020 3:41:25 AM</td>\n",
       "      <td>Lookup_unhandled</td>\n",
       "      <td>nullnull</td>\n",
       "      <td></td>\n",
       "      <td>Big Sis V3</td>\n",
       "      <td>xplore</td>\n",
       "      <td>6901e04c-929b-4ab9-bd8f-20c17a39c823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82135</th>\n",
       "      <td>10/22/2020 3:40:57 AM</td>\n",
       "      <td>Lookup_unhandled</td>\n",
       "      <td>nullnull</td>\n",
       "      <td></td>\n",
       "      <td>Big Sis V3</td>\n",
       "      <td>xplore</td>\n",
       "      <td>6901e04c-929b-4ab9-bd8f-20c17a39c823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82136</th>\n",
       "      <td>10/22/2020 3:37:46 AM</td>\n",
       "      <td>Lookup_unhandled</td>\n",
       "      <td>nullnull</td>\n",
       "      <td></td>\n",
       "      <td>Big Sis V3</td>\n",
       "      <td>ABS 4</td>\n",
       "      <td>e52d3733-0934-4a5b-9897-e6f5434b2dbb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82137 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Created On         Flow Name Merged Code  \\\n",
       "0      5/27/2020 10:31:06 PM        OB_NewUser         NaN   \n",
       "1       5/27/2020 7:38:52 PM      HLP_Feedback         NaN   \n",
       "2       5/27/2020 7:07:28 PM        OB_NewUser         NaN   \n",
       "3       5/27/2020 6:34:17 PM      HLP_Feedback         NaN   \n",
       "4       5/27/2020 5:53:14 PM      HLP_Feedback         NaN   \n",
       "...                      ...               ...         ...   \n",
       "82132  10/22/2020 4:00:18 AM  Lookup_unhandled    nullnull   \n",
       "82133  10/22/2020 3:42:48 AM  Lookup_unhandled    nullnull   \n",
       "82134  10/22/2020 3:41:25 AM  Lookup_unhandled    nullnull   \n",
       "82135  10/22/2020 3:40:57 AM  Lookup_unhandled    nullnull   \n",
       "82136  10/22/2020 3:37:46 AM  Lookup_unhandled    nullnull   \n",
       "\n",
       "                                            Message    Org Name  \\\n",
       "0                                           kya kru   Chhaa Jaa   \n",
       "1      like nand think improve sex topics nbut best  Big Sis V3   \n",
       "2                                               nan   Chhaa Jaa   \n",
       "3                                   nihaiti bakwaas   Chhaa Jaa   \n",
       "4                 mujhe apse bhut kuch seekhne mila   Chhaa Jaa   \n",
       "...                                             ...         ...   \n",
       "82132                                         topic  Big Sis V3   \n",
       "82133                                         thank  Big Sis V3   \n",
       "82134                                                Big Sis V3   \n",
       "82135                                                Big Sis V3   \n",
       "82136                                                Big Sis V3   \n",
       "\n",
       "                   Referred Flow                                  Uuid  \n",
       "0                            NaN  915617a9-c0ae-47fc-8df0-54d72d4f41d9  \n",
       "1                            NaN  148d099c-54eb-4182-961c-068eeda96691  \n",
       "2                            NaN  b05c8625-c18e-4f3a-9054-f7260f2541c5  \n",
       "3                            NaN  afd1ead9-90b8-40c5-8fd6-b543a9a6067b  \n",
       "4                            NaN  ddb1be9b-623d-4892-88c1-e441bf983103  \n",
       "...                          ...                                   ...  \n",
       "82132  NEW MENU_heart of big sis  19491afa-307d-4ca2-b04a-89f5f21651fb  \n",
       "82133                FP_MS1_MSSA  cf79a515-0e43-480f-9885-3c90eac8fd86  \n",
       "82134                     xplore  6901e04c-929b-4ab9-bd8f-20c17a39c823  \n",
       "82135                     xplore  6901e04c-929b-4ab9-bd8f-20c17a39c823  \n",
       "82136                      ABS 4  e52d3733-0934-4a5b-9897-e6f5434b2dbb  \n",
       "\n",
       "[82137 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbots_data_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#View cleaned data, all 80,000+ rows\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "chatbots_data_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('always hurt sec', 'always hurt sec'),\n",
       " ('get horny', 'always get wet'),\n",
       " ('loves someone else', 'loves someone else'),\n",
       " ('reset', 'reset'),\n",
       " ('talk representative', 'agent'),\n",
       " ('needy', 'needy bee'),\n",
       " ('talk representative', 'cant underst'),\n",
       " ('goodbye conversation ends thank participating', 'talk representative'),\n",
       " ('cock', 'penis'),\n",
       " ('agent', 'cant underst')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collocation finder....might scrap later\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(chatbots_data_chunks[\"Message\"].values.reshape(-1, ))\n",
    "\n",
    "#bigrams that appear 3+ times\n",
    "finder.apply_freq_filter(2)\n",
    "\n",
    "# return the 10 n-grams with the highest PMI (pointwise mutual information)\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Backend portion...find the most common phrases \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range = (3,3)) \n",
    "X1 = vectorizer.fit_transform(chatbots_data_chunks[\"Message\"])  \n",
    "features = (vectorizer.get_feature_names()) \n",
    "#print(\"\\n\\nFeatures : \\n\", features) <-- if you want to see all the phrases\n",
    "#print(\"\\n\\nX1 : \\n\", X1.toarray())   <-- but I don't recommend running\n",
    "  \n",
    "# Applying TFIDF \n",
    "vectorizer = TfidfVectorizer(ngram_range = (3,3)) \n",
    "X2 = vectorizer.fit_transform(chatbots_data_chunks[\"Message\"]) \n",
    "scores = (X2.toarray()) \n",
    "#print(\"\\n\\nScores : \\n\", scores) <--- I don't recommend running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Words head : \n",
      "                              term        rank\n",
      "25795          much services cost  306.000000\n",
      "6879              change mind sex  123.004667\n",
      "41998               thank big sis   76.751841\n",
      "19235              know ready sex   65.209097\n",
      "42125              thanks big sis   54.765816\n",
      "16259                 hey big sis   53.509655\n",
      "27913             normal sex hurt   52.912278\n",
      "21907  long distance relationship   46.307602\n",
      "18622     know boyfriend cheating   41.293563\n",
      "36234              sex first time   38.243805\n",
      "25742               much know sex   38.205654\n",
      "12210              first time sex   35.978438\n",
      "48523                yass big sis   33.000000\n",
      "34387              right time sex   32.192785\n",
      "22714           love really exist   28.510398\n",
      "31543                poke big sis   28.000000\n",
      "1762        anyone available chat   28.000000\n",
      "20712                let talk sex   27.360898\n",
      "23368         make boyfriend love   27.028650\n",
      "36361  sex important relationship   26.208595\n"
     ]
    }
   ],
   "source": [
    "# Getting top 20 ranking features \n",
    "sums = X2.sum(axis = 0) \n",
    "data1 = [] \n",
    "for col, term in enumerate(features): \n",
    "    data1.append( (term, sums[0,col] )) \n",
    "ranking = pd.DataFrame(data1, columns = ['term','rank']) \n",
    "words = (ranking.sort_values('rank', ascending = False)) \n",
    "# Alternatively, put \"True\" to see the most uncommon trigrams\n",
    "\n",
    "common_phrases =  print(\"\\n\\nWords head : \\n\", words.head(20)) \n",
    "common_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backend part that allows fuzzy to work\n",
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "choices = chatbots_data_chunks['Message'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('much services cost', 100),\n",
       " ('much service cost', 97),\n",
       " ('cost services', 84),\n",
       " ('nthank services', 67),\n",
       " ('much cost', 67),\n",
       " ('ask phones services', 65),\n",
       " ('services offer', 62),\n",
       " ('services', 62),\n",
       " ('much know stis', 62),\n",
       " ('know much stis', 62)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# much services cost\n",
    "# it appears that users are curious about the services the bots provide, as well as how much those services might cost\n",
    "# however, fuzzywuzzy is also pulling \"STIs\" for \"services\" which could be an issue\n",
    "process.extract(\"much services cost\", choices, limit = 10, scorer = fuzz.token_sort_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('change mind sex', 100),\n",
       " ('chang mind sex', 97),\n",
       " ('change minde sex', 97),\n",
       " ('chañge miñd sex', 93),\n",
       " ('change mibd sex', 93),\n",
       " ('change mind sec', 93),\n",
       " ('change mind sez', 93),\n",
       " ('change mins sex', 93),\n",
       " ('cgange mind sex', 93),\n",
       " ('change myvmind sex', 91)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change mind sex\n",
    "process.extract(\"change mind sex\", choices, limit = 10, scorer = fuzz.token_sort_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('know ready sex', 100),\n",
       " ('know read sex', 96),\n",
       " ('know yiu ready sex', 88),\n",
       " ('know ready sleep', 87),\n",
       " ('know ready', 83),\n",
       " ('ready know', 83),\n",
       " ('know ready start sex', 82),\n",
       " ('would know ready sex', 82),\n",
       " ('need know sex', 81),\n",
       " ('know oral sex', 81)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#know ready sex\n",
    "process.extract(\"know ready sex\", choices, limit = 10, scorer = fuzz.token_sort_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plan ahead:\n",
    "# Cluster data with Levenshtein distance: i.e., \"change myvmind sex\" is matched to \"change mind sex\"\n",
    "# Find a better language identification algorithm, so bots don't mistake different languages as mispelled words\n",
    "# which users are asking the most about which subject?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package crubadan to /opt/conda/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/crubadan.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    kac\n",
       "1    sun\n",
       "2    min\n",
       "3    tgl\n",
       "4    sun\n",
       "Name: Message, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('crubadan')\n",
    "#N-Gram based categorization, identifying languages...which is very off...\n",
    "\n",
    "tc = TextCat()\n",
    "chatbots_data_chunks[\"Message\"][0:5].apply(tc.guess_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./.cache/pip/wheels/4a/64/25/2e1cedbf7513fecda8d396a0431f3a1e217f45f1ab565e8ce8/sparse_dot_topn-0.2.9-cp38-cp38-linux_x86_64.whl\n",
      "Requirement already satisfied: cython>=0.29.15 in /opt/conda/lib/python3.8/site-packages (from sparse_dot_topn) (0.29.21)\n",
      "Requirement already satisfied: setuptools>=18.0 in /opt/conda/lib/python3.8/site-packages (from sparse_dot_topn) (49.6.0.post20201009)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.8/site-packages (from sparse_dot_topn) (1.18.5)\n",
      "Requirement already satisfied: scipy>=1.2.3 in /opt/conda/lib/python3.8/site-packages (from sparse_dot_topn) (1.5.2)\n",
      "Installing collected packages: sparse-dot-topn\n",
      "Successfully installed sparse-dot-topn-0.2.9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "#!pip install sparse_dot_topn\n",
    "import sparse_dot_topn.sparse_dot_topn as ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backend string matching\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def ngrams(string, n=3):\n",
    "    string = re.sub(r'[,-./]|\\sBD',r'', string)\n",
    "    ngrams = zip(*[string[i:] for i in range(n)])\n",
    "    return [''.join(ngram) for ngram in ngrams]\n",
    "\n",
    "message = chatbots_data_chunks[\"Message\"]\n",
    "vectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams)\n",
    "tf_idf_matrix = vectorizer.fit_transform(message)\n",
    "\n",
    "def awesome_cossim_top(A, B, ntop, lower_bound=0):\n",
    "    A = A.tocsr()\n",
    "    B = B.tocsr()\n",
    "    M, _ = A.shape\n",
    "    _, N = B.shape\n",
    "    idx_dtype = np.int32\n",
    "    nnz_max = M*ntop\n",
    "    indptr = np.zeros(M+1, dtype=idx_dtype)\n",
    "    indices = np.zeros(nnz_max, dtype=idx_dtype)\n",
    "    data = np.zeros(nnz_max, dtype=A.dtype)\n",
    "    ct.sparse_dot_topn(\n",
    "        M, N, np.asarray(A.indptr, dtype=idx_dtype),\n",
    "        np.asarray(A.indices, dtype=idx_dtype),\n",
    "        A.data,\n",
    "        np.asarray(B.indptr, dtype=idx_dtype),\n",
    "        np.asarray(B.indices, dtype=idx_dtype),\n",
    "        B.data,\n",
    "        ntop,\n",
    "        lower_bound,\n",
    "        indptr, indices, data)\n",
    "    return csr_matrix((data,indices,indptr),shape=(M,N))\n",
    "\n",
    "import time\n",
    "t1 = time.time()\n",
    "matches = awesome_cossim_top(tf_idf_matrix, tf_idf_matrix.transpose(), 10, 0.8)\n",
    "t = time.time()-t1\n",
    "\n",
    "def get_matches_df(sparse_matrix, name_vector, top=100):\n",
    "    non_zeros = sparse_matrix.nonzero()\n",
    "    sparserows = non_zeros[0]\n",
    "    sparsecols = non_zeros[1]\n",
    "    if top:\n",
    "        nr_matches = top\n",
    "    else:\n",
    "        nr_matches = sparsecols.size\n",
    "    left_side = np.empty([nr_matches], dtype=object)\n",
    "    right_side = np.empty([nr_matches], dtype=object)\n",
    "    similairity = np.zeros(nr_matches)\n",
    "    \n",
    "    for index in range(0, nr_matches):\n",
    "        left_side[index] = name_vector[sparserows[index]]\n",
    "        right_side[index] = name_vector[sparsecols[index]]\n",
    "        similairity[index] = sparse_matrix.data[index]\n",
    "    \n",
    "    return pd.DataFrame({'left_side': left_side,\n",
    "                          'right_side': right_side,\n",
    "                           'similarity': similairity})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left_side</th>\n",
       "      <th>right_side</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96002</th>\n",
       "      <td>body shaming</td>\n",
       "      <td>body shame</td>\n",
       "      <td>0.818368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93516</th>\n",
       "      <td>need sex videos</td>\n",
       "      <td>sex videos</td>\n",
       "      <td>0.844718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53440</th>\n",
       "      <td>long take get pregnancy</td>\n",
       "      <td>long take get pregnant</td>\n",
       "      <td>0.926623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63713</th>\n",
       "      <td>teacher</td>\n",
       "      <td>teach</td>\n",
       "      <td>0.846122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56674</th>\n",
       "      <td>infections</td>\n",
       "      <td>infection</td>\n",
       "      <td>0.963932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81324</th>\n",
       "      <td>makes sensitive breasts</td>\n",
       "      <td>sensitive breast</td>\n",
       "      <td>0.827204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85376</th>\n",
       "      <td>sex feel like pregnant</td>\n",
       "      <td>yes feel like pregnant</td>\n",
       "      <td>0.831833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91237</th>\n",
       "      <td>know baby</td>\n",
       "      <td>want know baby</td>\n",
       "      <td>0.806466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68530</th>\n",
       "      <td>hot break virginity</td>\n",
       "      <td>one break virginity</td>\n",
       "      <td>0.836577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99539</th>\n",
       "      <td>relationship complicated</td>\n",
       "      <td>complicated relationships</td>\n",
       "      <td>0.837706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80307</th>\n",
       "      <td>want fall pregnant</td>\n",
       "      <td>fall pregnant fast</td>\n",
       "      <td>0.846347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63989</th>\n",
       "      <td>curiosity sec</td>\n",
       "      <td>curiosity sex</td>\n",
       "      <td>0.919976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6651</th>\n",
       "      <td>send photos</td>\n",
       "      <td>send photo</td>\n",
       "      <td>0.897497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60618</th>\n",
       "      <td>answered</td>\n",
       "      <td>answer</td>\n",
       "      <td>0.817300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50656</th>\n",
       "      <td>partner give attention sex</td>\n",
       "      <td>partner give attention</td>\n",
       "      <td>0.935758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24766</th>\n",
       "      <td>good behen</td>\n",
       "      <td>good bol behen</td>\n",
       "      <td>0.822111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95650</th>\n",
       "      <td>okay talk tomorrow</td>\n",
       "      <td>talk tomorrow</td>\n",
       "      <td>0.862895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32742</th>\n",
       "      <td>aap kaha</td>\n",
       "      <td>aap kahan</td>\n",
       "      <td>0.966350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57203</th>\n",
       "      <td>long distance relationship big sis</td>\n",
       "      <td>long distance relationship</td>\n",
       "      <td>0.865635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63954</th>\n",
       "      <td>sexually attracted gender</td>\n",
       "      <td>sexually attracted girls</td>\n",
       "      <td>0.818971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19435</th>\n",
       "      <td>thanks big</td>\n",
       "      <td>thanks big sis</td>\n",
       "      <td>0.849610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25106</th>\n",
       "      <td>satisfy man bed</td>\n",
       "      <td>satisfy man sex</td>\n",
       "      <td>0.807309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98577</th>\n",
       "      <td>want sex virgin</td>\n",
       "      <td>sex virgin</td>\n",
       "      <td>0.848679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94707</th>\n",
       "      <td>see cheating boyfriend</td>\n",
       "      <td>cheating boyfriend</td>\n",
       "      <td>0.857796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53804</th>\n",
       "      <td>love sex smarts</td>\n",
       "      <td>sex smarts</td>\n",
       "      <td>0.879617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93216</th>\n",
       "      <td>happens need real life councillor</td>\n",
       "      <td>need real life councillor</td>\n",
       "      <td>0.866058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22240</th>\n",
       "      <td>birth control advice</td>\n",
       "      <td>birth control</td>\n",
       "      <td>0.801380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95215</th>\n",
       "      <td>mornin big sis</td>\n",
       "      <td>morning big sis</td>\n",
       "      <td>0.805511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49780</th>\n",
       "      <td>sex miscarriage</td>\n",
       "      <td>miscarriage</td>\n",
       "      <td>0.891930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53488</th>\n",
       "      <td>define vaginal</td>\n",
       "      <td>define vagina</td>\n",
       "      <td>0.955266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59271</th>\n",
       "      <td>get depression</td>\n",
       "      <td>depression</td>\n",
       "      <td>0.855777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62054</th>\n",
       "      <td>okay thanks helping</td>\n",
       "      <td>thanks helping</td>\n",
       "      <td>0.855348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32743</th>\n",
       "      <td>aap kaha</td>\n",
       "      <td>aap kahan</td>\n",
       "      <td>0.966350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72760</th>\n",
       "      <td>good long distance relationship</td>\n",
       "      <td>long distance relationship</td>\n",
       "      <td>0.905689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17104</th>\n",
       "      <td>kya hall hai</td>\n",
       "      <td>kya hal hai</td>\n",
       "      <td>0.915160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90763</th>\n",
       "      <td>commubicating partner test knowledge</td>\n",
       "      <td>communicating partner test knowledge</td>\n",
       "      <td>0.823525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92299</th>\n",
       "      <td>negotiate safe sex</td>\n",
       "      <td>negotiating safe sex</td>\n",
       "      <td>0.887518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75974</th>\n",
       "      <td>make sex interesting</td>\n",
       "      <td>make sex interesting partner</td>\n",
       "      <td>0.840432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58944</th>\n",
       "      <td>know right time sex</td>\n",
       "      <td>right time sex</td>\n",
       "      <td>0.831104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85564</th>\n",
       "      <td>kiss important relationship</td>\n",
       "      <td>important relationship</td>\n",
       "      <td>0.838909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99872</th>\n",
       "      <td>ouch sex hurt</td>\n",
       "      <td>ouch sex hurts</td>\n",
       "      <td>0.936329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29013</th>\n",
       "      <td>pregnant want pregnant</td>\n",
       "      <td>want pregnant</td>\n",
       "      <td>0.921304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96401</th>\n",
       "      <td>fall pregnant</td>\n",
       "      <td>fall pregnant sex</td>\n",
       "      <td>0.885497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86592</th>\n",
       "      <td>always struggling understand</td>\n",
       "      <td>struggling understand</td>\n",
       "      <td>0.866496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36740</th>\n",
       "      <td>get rid discharge</td>\n",
       "      <td>get rid discharge smell</td>\n",
       "      <td>0.846409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66057</th>\n",
       "      <td>feel break virginity</td>\n",
       "      <td>break virginity</td>\n",
       "      <td>0.853813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19741</th>\n",
       "      <td>need communicating partner</td>\n",
       "      <td>communicating partner</td>\n",
       "      <td>0.891380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70985</th>\n",
       "      <td>something specific</td>\n",
       "      <td>looking something specific</td>\n",
       "      <td>0.858674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58363</th>\n",
       "      <td>complicated relationship</td>\n",
       "      <td>iam complicated relationship</td>\n",
       "      <td>0.873468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71243</th>\n",
       "      <td>let chat tomorrow</td>\n",
       "      <td>chat tomorrow</td>\n",
       "      <td>0.882735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  left_side  \\\n",
       "96002                          body shaming   \n",
       "93516                       need sex videos   \n",
       "53440               long take get pregnancy   \n",
       "63713                               teacher   \n",
       "56674                            infections   \n",
       "81324               makes sensitive breasts   \n",
       "85376                sex feel like pregnant   \n",
       "91237                             know baby   \n",
       "68530                   hot break virginity   \n",
       "99539              relationship complicated   \n",
       "80307                    want fall pregnant   \n",
       "63989                         curiosity sec   \n",
       "6651                            send photos   \n",
       "60618                              answered   \n",
       "50656            partner give attention sex   \n",
       "24766                            good behen   \n",
       "95650                    okay talk tomorrow   \n",
       "32742                              aap kaha   \n",
       "57203    long distance relationship big sis   \n",
       "63954             sexually attracted gender   \n",
       "19435                            thanks big   \n",
       "25106                       satisfy man bed   \n",
       "98577                       want sex virgin   \n",
       "94707                see cheating boyfriend   \n",
       "53804                       love sex smarts   \n",
       "93216     happens need real life councillor   \n",
       "22240                  birth control advice   \n",
       "95215                        mornin big sis   \n",
       "49780                       sex miscarriage   \n",
       "53488                        define vaginal   \n",
       "59271                        get depression   \n",
       "62054                   okay thanks helping   \n",
       "32743                              aap kaha   \n",
       "72760       good long distance relationship   \n",
       "17104                          kya hall hai   \n",
       "90763  commubicating partner test knowledge   \n",
       "92299                    negotiate safe sex   \n",
       "75974                  make sex interesting   \n",
       "58944                   know right time sex   \n",
       "85564           kiss important relationship   \n",
       "99872                         ouch sex hurt   \n",
       "29013                pregnant want pregnant   \n",
       "96401                         fall pregnant   \n",
       "86592          always struggling understand   \n",
       "36740                     get rid discharge   \n",
       "66057                  feel break virginity   \n",
       "19741            need communicating partner   \n",
       "70985                    something specific   \n",
       "58363              complicated relationship   \n",
       "71243                     let chat tomorrow   \n",
       "\n",
       "                                 right_side  similarity  \n",
       "96002                            body shame    0.818368  \n",
       "93516                            sex videos    0.844718  \n",
       "53440                long take get pregnant    0.926623  \n",
       "63713                                 teach    0.846122  \n",
       "56674                             infection    0.963932  \n",
       "81324                      sensitive breast    0.827204  \n",
       "85376                yes feel like pregnant    0.831833  \n",
       "91237                        want know baby    0.806466  \n",
       "68530                   one break virginity    0.836577  \n",
       "99539             complicated relationships    0.837706  \n",
       "80307                    fall pregnant fast    0.846347  \n",
       "63989                         curiosity sex    0.919976  \n",
       "6651                             send photo    0.897497  \n",
       "60618                                answer    0.817300  \n",
       "50656                partner give attention    0.935758  \n",
       "24766                        good bol behen    0.822111  \n",
       "95650                         talk tomorrow    0.862895  \n",
       "32742                             aap kahan    0.966350  \n",
       "57203            long distance relationship    0.865635  \n",
       "63954              sexually attracted girls    0.818971  \n",
       "19435                        thanks big sis    0.849610  \n",
       "25106                       satisfy man sex    0.807309  \n",
       "98577                            sex virgin    0.848679  \n",
       "94707                    cheating boyfriend    0.857796  \n",
       "53804                            sex smarts    0.879617  \n",
       "93216             need real life councillor    0.866058  \n",
       "22240                         birth control    0.801380  \n",
       "95215                       morning big sis    0.805511  \n",
       "49780                           miscarriage    0.891930  \n",
       "53488                         define vagina    0.955266  \n",
       "59271                            depression    0.855777  \n",
       "62054                        thanks helping    0.855348  \n",
       "32743                             aap kahan    0.966350  \n",
       "72760            long distance relationship    0.905689  \n",
       "17104                           kya hal hai    0.915160  \n",
       "90763  communicating partner test knowledge    0.823525  \n",
       "92299                  negotiating safe sex    0.887518  \n",
       "75974          make sex interesting partner    0.840432  \n",
       "58944                        right time sex    0.831104  \n",
       "85564                important relationship    0.838909  \n",
       "99872                        ouch sex hurts    0.936329  \n",
       "29013                         want pregnant    0.921304  \n",
       "96401                     fall pregnant sex    0.885497  \n",
       "86592                 struggling understand    0.866496  \n",
       "36740               get rid discharge smell    0.846409  \n",
       "66057                       break virginity    0.853813  \n",
       "19741                 communicating partner    0.891380  \n",
       "70985            looking something specific    0.858674  \n",
       "58363          iam complicated relationship    0.873468  \n",
       "71243                         chat tomorrow    0.882735  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#string-matching\n",
    "matches_df = get_matches_df(matches, message, top=100000)\n",
    "matches_df = matches_df[matches_df['similarity'] < 0.99999] # Remove all exact matches\n",
    "matches_df.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Pandas dataframe to Table (where you can use table.select(...)). Not suggested -- takes up lots of memory\n",
    "chatbots_data = Table.from_df(chatbots_data_chunks, keep_index=False)\n",
    "chatbots_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
