{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk \n",
    "from nltk.classify.textcat import TextCat \n",
    "from matplotlib.pyplot import plot \n",
    "from datascience import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fuzzywuzzy\n",
    "!pip3 install fuzzywuzzy[speedup]\n",
    "!pip install python-Levenshtein\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "#(py)torch\n",
    "!pip install torch\n",
    "\n",
    "#langid (language identification)\n",
    "!pip install langid\n",
    "import langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#DO NOT RUN \n",
    "#This code creates a traditional table as seen in the datascience * package but takes up too much memory. \n",
    "#I suggest running the code block after this one that generates chunks of 10 for each row of data.\n",
    "\n",
    "chatbots_data = Table.read_table(\"chatbots_data.csv\")\n",
    "\n",
    "#Convert Pandas dataframe to Table (where you can use table.select(...)). Not suggested -- takes up lots of memory\n",
    "chatbots_data = Table.from_df(chatbots_data_chunks, keep_index=False)\n",
    "\n",
    "chatbots_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dividing the data into chunks of 10, because of memory issues (kernel keeps dying)\n",
    "#for example, this is what one chunk looks like\n",
    "chunksize = 10\n",
    "for chunk in pd.read_csv(\"chatbots_data.csv\", chunksize = chunksize):\n",
    "    print(chunk.shape)\n",
    "    print(\"=\"*66)\n",
    "    print(chunk.head(2))\n",
    "    print(\"=\"*66)\n",
    "    break\n",
    "    \n",
    "#appending chunks to chatbot_list, an empty array\n",
    "chatbot_list = []\n",
    "chunksize = 10\n",
    "for chunk in pd.read_csv(\"chatbots_data.csv\", chunksize = chunksize):\n",
    "    chatbot_list.append(chunk)\n",
    "\n",
    "#Combine all the chunks into one dataframe\n",
    "chatbots_data_chunks = pd.concat(chatbot_list, axis=0)\n",
    "chatbots_data_chunks = pd.DataFrame(data = chatbots_data_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleans the \"Message\" column\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "def preprocess(sentence):\n",
    "    sentence=str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    sentence=sentence.replace('{html}',\"\").replace('_',\"\")\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(rem_num)  \n",
    "    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
    "    stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "chatbots_data_chunks['Message']=chatbots_data_chunks['Message'].map(lambda s:preprocess(s)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view chunks\n",
    "chatbots_data_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#View cleaned data, all 80,000+ rows\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "chatbots_data_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collocation finder....might scrap later\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(chatbots_data_chunks[\"Message\"].values.reshape(-1, ))\n",
    "\n",
    "bigrams that appear 3+ times\n",
    "finder.apply_freq_filter(2)\n",
    "\n",
    " return the 10 n-grams with the highest PMI (pointwise mutual information)\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Backend portion...find the most common phrases \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range = (3,3)) \n",
    "X1 = vectorizer.fit_transform(chatbots_data_chunks[\"Message\"])  \n",
    "features = (vectorizer.get_feature_names()) \n",
    "#print(\"\\n\\nFeatures : \\n\", features) <-- if you want to see all the phrases\n",
    "#print(\"\\n\\nX1 : \\n\", X1.toarray())   <-- but I don't recommend running\n",
    "  \n",
    "# Applying TFIDF \n",
    "vectorizer = TfidfVectorizer(ngram_range = (3,3)) \n",
    "X2 = vectorizer.fit_transform(chatbots_data_chunks[\"Message\"]) \n",
    "scores = (X2.toarray()) \n",
    "#print(\"\\n\\nScores : \\n\", scores) <--- I don't recommend running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting top 20 ranking features \n",
    "sums = X2.sum(axis = 0) \n",
    "data1 = [] \n",
    "for col, term in enumerate(features): \n",
    "    data1.append( (term, sums[0,col] )) \n",
    "ranking = pd.DataFrame(data1, columns = ['term','rank']) \n",
    "words = (ranking.sort_values('rank', ascending = False)) \n",
    "# Alternatively, put \"True\" here to see the most uncommon trigrams\n",
    "\n",
    "common_phrases =  print(\"\\n\\nWords head : \\n\", words.head(20)) \n",
    "common_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backend part that allows fuzzy to work\n",
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "choices = chatbots_data_chunks['Message'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# much services cost\n",
    "# it appears that users are curious about the services the bots provide, as well as how much those services might cost\n",
    "# however, fuzzywuzzy is also pulling \"STIs\" for \"services\" which could be an issue\n",
    "process.extract(\"much services cost\", choices, limit = 10, scorer = fuzz.token_sort_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change mind sex\n",
    "process.extract(\"change mind sex\", choices, limit = 10, scorer = fuzz.token_sort_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#know ready sex\n",
    "process.extract(\"know ready sex\", choices, limit = 10, scorer = fuzz.token_sort_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('crubadan')\n",
    "#N-Gram based categorization, identifying languages...which is very off...\n",
    "\n",
    "tc = TextCat()\n",
    "chatbots_data_chunks[\"Message\"][0:5].apply(tc.guess_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from scipy.sparse import csr_matrix\n",
    "!pip install sparse_dot_topn\n",
    "import sparse_dot_topn.sparse_dot_topn as ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backend portion for string matching\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def ngrams(string, n=3):\n",
    "    string = re.sub(r'[,-./]|\\sBD',r'', string)\n",
    "    ngrams = zip(*[string[i:] for i in range(n)])\n",
    "    return [''.join(ngram) for ngram in ngrams]\n",
    "\n",
    "message = chatbots_data_chunks[\"Message\"]\n",
    "vectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams)\n",
    "tf_idf_matrix = vectorizer.fit_transform(message)\n",
    "\n",
    "def awesome_cossim_top(A, B, ntop, lower_bound=0):\n",
    "    A = A.tocsr()\n",
    "    B = B.tocsr()\n",
    "    M, _ = A.shape\n",
    "    _, N = B.shape\n",
    "    idx_dtype = np.int32\n",
    "    nnz_max = M*ntop\n",
    "    indptr = np.zeros(M+1, dtype=idx_dtype)\n",
    "    indices = np.zeros(nnz_max, dtype=idx_dtype)\n",
    "    data = np.zeros(nnz_max, dtype=A.dtype)\n",
    "    ct.sparse_dot_topn(\n",
    "        M, N, np.asarray(A.indptr, dtype=idx_dtype),\n",
    "        np.asarray(A.indices, dtype=idx_dtype),\n",
    "        A.data,\n",
    "        np.asarray(B.indptr, dtype=idx_dtype),\n",
    "        np.asarray(B.indices, dtype=idx_dtype),\n",
    "        B.data,\n",
    "        ntop,\n",
    "        lower_bound,\n",
    "        indptr, indices, data)\n",
    "    return csr_matrix((data,indices,indptr),shape=(M,N))\n",
    "\n",
    "import time\n",
    "t1 = time.time()\n",
    "matches = awesome_cossim_top(tf_idf_matrix, tf_idf_matrix.transpose(), 10, 0.8)\n",
    "t = time.time()-t1\n",
    "\n",
    "def get_matches_df(sparse_matrix, name_vector, top=100):\n",
    "    non_zeros = sparse_matrix.nonzero()\n",
    "    sparserows = non_zeros[0]\n",
    "    sparsecols = non_zeros[1]\n",
    "    if top:\n",
    "        nr_matches = top\n",
    "    else:\n",
    "        nr_matches = sparsecols.size\n",
    "    left_side = np.empty([nr_matches], dtype=object)\n",
    "    right_side = np.empty([nr_matches], dtype=object)\n",
    "    similairity = np.zeros(nr_matches)\n",
    "    \n",
    "    for index in range(0, nr_matches):\n",
    "        left_side[index] = name_vector[sparserows[index]]\n",
    "        right_side[index] = name_vector[sparsecols[index]]\n",
    "        similairity[index] = sparse_matrix.data[index]\n",
    "    \n",
    "    return pd.DataFrame({'left_side': left_side,\n",
    "                          'right_side': right_side,\n",
    "                           'similarity': similairity})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#string-matching output\n",
    "matches_df = get_matches_df(matches, message, top=100000)\n",
    "matches_df = matches_df[matches_df['similarity'] < 0.99999] # Remove all exact matches\n",
    "matches_df.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plan ahead:\n",
    "# Cluster data with Levenshtein distance: i.e., \"change myvmind sex\" is matched to \"change mind sex\"\n",
    "# Find a better language identification algorithm, so bots don't mistake different languages as mispelled words\n",
    "# Q: Which users are asking the most about which subject?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
